{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cloud_build_tfx.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOxq4JKmqj6ysgsgEK4LuzW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/CI-CD-for-Model-Training/blob/main/cloud_build_tfx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j59AaHsxePSW"
      },
      "source": [
        "## References\n",
        "\n",
        "* https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training\n",
        "* https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tR7ZgI4DCPz"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DDXrX_Ee359"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oy0ymAXdXKA"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhp1xhz6B1u5"
      },
      "source": [
        "GOOGLE_CLOUD_PROJECT = 'fast-ai-exploration'    \n",
        "GOOGLE_CLOUD_REGION = 'us-central1'      \n",
        "GCS_BUCKET_NAME = 'vertex-tfx-mlops'     \n",
        "\n",
        "PIPELINE_NAME = 'penguin-vertex-training'\n",
        "DATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
        "    from absl import logging\n",
        "    logging.error('Please set all required parameters.')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu5Ojm61fH-S",
        "outputId": "cc98cd4b-5146-4089-8cec-756844b4c88c"
      },
      "source": [
        "!gsutil cp gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv {DATA_ROOT}/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 25.0 KiB/ 25.0 KiB]                                                \n",
            "Operation completed over 1 objects/25.0 KiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkMnLHuGDGXW"
      },
      "source": [
        "## Training module for TFX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6L7hhCufOeD"
      },
      "source": [
        "_trainer_module_file = 'penguin_trainer.py'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjAFV2szfRT7",
        "outputId": "0e0ecc5c-c82e-4d5b-8005-b9b7e65e710c"
      },
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and\n",
        "# slightly modified run_fn() to add distribution_strategy.\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "_FEATURE_KEYS = [\n",
        "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
        "]\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "# Since we're not generating or creating a schema, we will instead create\n",
        "# a feature spec.  Since there are a fairly small number of features this is\n",
        "# manageable for this dataset.\n",
        "_FEATURE_SPEC = {\n",
        "    **{\n",
        "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
        "        for feature in _FEATURE_KEYS\n",
        "    }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[str],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              schema: schema_pb2.Schema,\n",
        "              batch_size: int) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    schema: schema of the input data.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      schema=schema).repeat()\n",
        "\n",
        "\n",
        "def _make_keras_model() -> tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model.\n",
        "  \"\"\"\n",
        "  # The model below is built with Functional API, please refer to\n",
        "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
        "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3)(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# NEW: Read `use_gpu` from the custom_config of the Trainer.\n",
        "#      if it uses GPU, enable MirroredStrategy.\n",
        "def _get_distribution_strategy(fn_args: tfx.components.FnArgs):\n",
        "  if fn_args.custom_config.get('use_gpu', False):\n",
        "    logging.info('Using MirroredStrategy with one GPU.')\n",
        "    return tf.distribute.MirroredStrategy(devices=['device:GPU:0'])\n",
        "  return None\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
        "  # version provided by pipeline author. A schema can also derived from TFT\n",
        "  # graph if a Transform component is used. In the case when either is missing,\n",
        "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
        "  # feature_spec, but the schema returned would be very primitive.\n",
        "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  # NEW: If we have a distribution strategy, build a model in a strategy scope.\n",
        "  strategy = _get_distribution_strategy(fn_args)\n",
        "  if strategy is None:\n",
        "    model = _make_keras_model()\n",
        "  else:\n",
        "    with strategy.scope():\n",
        "      model = _make_keras_model()\n",
        "\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps,\n",
        "      epochs=1)\n",
        "\n",
        "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
        "  # directory.\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting penguin_trainer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khip4KnDfWAw",
        "outputId": "1415cf0a-c3da-41c0-cb4d-c4f0b27a52e8"
      },
      "source": [
        "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://penguin_trainer.py [Content-Type=text/x-python]...\n",
            "/ [0 files][    0.0 B/  4.4 KiB]                                                \r/ [1 files][  4.4 KiB/  4.4 KiB]                                                \r\n",
            "Operation completed over 1 objects/4.4 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoeUHbOAJCrz"
      },
      "source": [
        "## Build an image for CI/CD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgwNU2yQIBTV",
        "outputId": "43f2f413-9e58-4f25-befd-be4c4cee4d81"
      },
      "source": [
        "# %%writefile Dockerfile\n",
        "\n",
        "# FROM gcr.io/tfx-oss-public/tfx:1.0.0\n",
        "\n",
        "# RUN pip install -U pip\n",
        "# RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing Dockerfile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbaapcYVIPlp"
      },
      "source": [
        "# CICD_IMAGE_NAME = 'cicd:latest'\n",
        "# CICD_IMAGE_URI = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{CICD_IMAGE_NAME}\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVUOrlqIfZ4"
      },
      "source": [
        "# !gcloud builds submit --tag $CICD_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6jgBaknDJaz"
      },
      "source": [
        "## Cloud Build configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqNAjXJO7aHH"
      },
      "source": [
        "REPO_URL = \"https://github.com/sayakpaul/CI-CD-for-Model-Training\" \n",
        "BRANCH = \"main\"\n",
        "\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "CICD_IMAGE_URI = 'gcr.io/tfx-oss-public/tfx:1.0.0'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZcjZcO87L3A",
        "outputId": "290060e4-05cd-49ab-dd79-25b73bcb1671"
      },
      "source": [
        "SUBSTITUTIONS=f\"\"\"\\\n",
        "_REPO_URL='{REPO_URL}',\\\n",
        "_BRANCH={BRANCH},\\\n",
        "_PROJECT={GOOGLE_CLOUD_PROJECT},\\\n",
        "_REGION={GOOGLE_CLOUD_REGION},\\\n",
        "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
        "_PIPELINE_ROOT={PIPELINE_ROOT},\\\n",
        "_MODULE_ROOT={MODULE_ROOT},\\\n",
        "_DATA_ROOT={DATA_ROOT},\\\n",
        "_SERVING_MODEL_DIR={SERVING_MODEL_DIR},\\\n",
        "_CICD_IMAGE_URI={CICD_IMAGE_URI}\n",
        "\"\"\"\n",
        "\n",
        "!echo $SUBSTITUTIONS"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_REPO_URL=https://github.com/sayakpaul/CI-CD-for-Model-Training,_BRANCH=main,_PROJECT=fast-ai-exploration,_REGION=us-central1,_PIPELINE_NAME=penguin-vertex-training,_PIPELINE_ROOT=gs://vertex-tfx-mlops/pipeline_root/penguin-vertex-training,_MODULE_ROOT=gs://vertex-tfx-mlops/pipeline_module/penguin-vertex-training,_DATA_ROOT=gs://vertex-tfx-mlops/data/penguin-vertex-training,_SERVING_MODEL_DIR=gs://vertex-tfx-mlops/serving_model/penguin-vertex-training,_CICD_IMAGE_URI=gcr.io/tfx-oss-public/tfx:1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEjWZxeNDN41"
      },
      "source": [
        "## Submit to Cloud Build\n",
        "\n",
        "The output of Cloud Build, in this case, is a compiled pipeline uploaded to GCS Bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plY19wz89cK_"
      },
      "source": [
        "!git clone https://github.com/sayakpaul/CI-CD-for-Model-Training --quiet"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AldFybUB8P22",
        "outputId": "6daeab39-fd55-4635-836b-8c9ca4c2554b"
      },
      "source": [
        "!gcloud builds submit --no-source --timeout=60m \\\n",
        "    --config CI-CD-for-Model-Training/build/pipeline-deployment.yaml \\\n",
        "    --substitutions {SUBSTITUTIONS} \\\n",
        "    --machine-type=e2-highcpu-8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created [https://cloudbuild.googleapis.com/v1/projects/fast-ai-exploration/locations/global/builds/868cad2f-a902-498c-a3df-6390afdd6eed].\n",
            "Logs are available at [https://console.cloud.google.com/cloud-build/builds/868cad2f-a902-498c-a3df-6390afdd6eed?project=29880397572].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"868cad2f-a902-498c-a3df-6390afdd6eed\"\n",
            "\n",
            "FETCHSOURCE\n",
            "BUILD\n",
            "Starting Step #0 - \"Clone Repository\"\n",
            "Step #0 - \"Clone Repository\": Already have image (with digest): gcr.io/cloud-builders/git\n",
            "Step #0 - \"Clone Repository\": Cloning into 'CI-CD-for-Model-Training'...\n",
            "Step #0 - \"Clone Repository\": POST git-upload-pack (341 bytes)\n",
            "Step #0 - \"Clone Repository\": POST git-upload-pack (194 bytes)\n",
            "Finished Step #0 - \"Clone Repository\"\n",
            "Starting Step #1 - \"Compile Pipeline\"\n",
            "Step #1 - \"Compile Pipeline\": Pulling image: gcr.io/tfx-oss-public/tfx:1.0.0\n",
            "Step #1 - \"Compile Pipeline\": 1.0.0: Pulling from tfx-oss-public/tfx\n",
            "Step #1 - \"Compile Pipeline\": 25fa05cd42bd: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 2d6e353a95ec: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 14d7996407de: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 0c9c6fc70f16: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": c3c76be11512: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": ab6e5a9c78ee: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 7bc1690abd59: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": f5b4dd7682bc: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": d6897660f71d: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 174d792fb622: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": a7a7ae3235b8: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": e4790a266767: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": daba7cb3f799: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 55d5ef773782: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 52771a439fa8: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 810d1f8c7f15: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 6e1d6dba0d16: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": b078c2022d55: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 69577f1d702f: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 9137b84f7e19: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 98990c035a71: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 582d3e1b0799: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 123a5c227305: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": e1a94bcd15d3: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 9a991d5f7266: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 45439985d602: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 673e674e91bd: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": ecc3ce1cf47f: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 75df750e0c48: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 39347a0726ae: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 39705430dced: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 195d3ff7189f: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 6657d01ec620: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 273173b54e2b: Pulling fs layer\n",
            "Step #1 - \"Compile Pipeline\": 7bc1690abd59: Waiting\n",
            "Step #1 - \"Compile Pipeline\": f5b4dd7682bc: Waiting\n",
            "Step #1 - \"Compile Pipeline\": d6897660f71d: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 174d792fb622: Waiting\n",
            "Step #1 - \"Compile Pipeline\": a7a7ae3235b8: Waiting\n",
            "Step #1 - \"Compile Pipeline\": e4790a266767: Waiting\n",
            "Step #1 - \"Compile Pipeline\": daba7cb3f799: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 55d5ef773782: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 52771a439fa8: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 810d1f8c7f15: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 6e1d6dba0d16: Waiting\n",
            "Step #1 - \"Compile Pipeline\": b078c2022d55: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 69577f1d702f: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 9137b84f7e19: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 98990c035a71: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 582d3e1b0799: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 123a5c227305: Waiting\n",
            "Step #1 - \"Compile Pipeline\": e1a94bcd15d3: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 9a991d5f7266: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 45439985d602: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 673e674e91bd: Waiting\n",
            "Step #1 - \"Compile Pipeline\": ecc3ce1cf47f: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 75df750e0c48: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 39347a0726ae: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 39705430dced: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 195d3ff7189f: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 6657d01ec620: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 273173b54e2b: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 0c9c6fc70f16: Waiting\n",
            "Step #1 - \"Compile Pipeline\": c3c76be11512: Waiting\n",
            "Step #1 - \"Compile Pipeline\": ab6e5a9c78ee: Waiting\n",
            "Step #1 - \"Compile Pipeline\": 2d6e353a95ec: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 2d6e353a95ec: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 14d7996407de: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 14d7996407de: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 0c9c6fc70f16: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 0c9c6fc70f16: Download complete\n",
            "Step #1 - \"Compile Pipeline\": c3c76be11512: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": c3c76be11512: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 25fa05cd42bd: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 25fa05cd42bd: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 7bc1690abd59: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 7bc1690abd59: Download complete\n",
            "Step #1 - \"Compile Pipeline\": d6897660f71d: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": d6897660f71d: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 25fa05cd42bd: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 2d6e353a95ec: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 14d7996407de: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 0c9c6fc70f16: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": c3c76be11512: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": f5b4dd7682bc: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": f5b4dd7682bc: Download complete\n",
            "Step #1 - \"Compile Pipeline\": a7a7ae3235b8: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 174d792fb622: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 174d792fb622: Download complete\n",
            "Step #1 - \"Compile Pipeline\": ab6e5a9c78ee: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": ab6e5a9c78ee: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 55d5ef773782: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 55d5ef773782: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 52771a439fa8: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 52771a439fa8: Download complete\n",
            "Step #1 - \"Compile Pipeline\": daba7cb3f799: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 6e1d6dba0d16: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 6e1d6dba0d16: Download complete\n",
            "Step #1 - \"Compile Pipeline\": e4790a266767: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": e4790a266767: Download complete\n",
            "Step #1 - \"Compile Pipeline\": b078c2022d55: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": b078c2022d55: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 9137b84f7e19: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 9137b84f7e19: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 69577f1d702f: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 69577f1d702f: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 582d3e1b0799: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 582d3e1b0799: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 98990c035a71: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 98990c035a71: Download complete\n",
            "Step #1 - \"Compile Pipeline\": e1a94bcd15d3: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": e1a94bcd15d3: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 123a5c227305: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 123a5c227305: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 810d1f8c7f15: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 810d1f8c7f15: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 673e674e91bd: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 673e674e91bd: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 9a991d5f7266: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 9a991d5f7266: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 75df750e0c48: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 75df750e0c48: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 39347a0726ae: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 39347a0726ae: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 39705430dced: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 39705430dced: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 195d3ff7189f: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 195d3ff7189f: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 6657d01ec620: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 6657d01ec620: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 273173b54e2b: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 273173b54e2b: Download complete\n",
            "Step #1 - \"Compile Pipeline\": ecc3ce1cf47f: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": ecc3ce1cf47f: Download complete\n",
            "Step #1 - \"Compile Pipeline\": 45439985d602: Verifying Checksum\n",
            "Step #1 - \"Compile Pipeline\": 45439985d602: Download complete\n",
            "Step #1 - \"Compile Pipeline\": ab6e5a9c78ee: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 7bc1690abd59: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": f5b4dd7682bc: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": d6897660f71d: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 174d792fb622: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": a7a7ae3235b8: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": e4790a266767: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": daba7cb3f799: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 55d5ef773782: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 52771a439fa8: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 810d1f8c7f15: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 6e1d6dba0d16: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": b078c2022d55: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 69577f1d702f: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 9137b84f7e19: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 98990c035a71: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 582d3e1b0799: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 123a5c227305: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": e1a94bcd15d3: Pull complete\n",
            "Step #1 - \"Compile Pipeline\": 9a991d5f7266: Pull complete\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}